{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j65i49dU8Cjs",
        "outputId": "4d33dbe6-cae8-44e8-9857-d84d8f4339c5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\", force_remount=True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zVeC5Mf8C-S"
      },
      "source": [
        "from torch.utils.data import (DataLoader, TensorDataset)\n",
        "from torch import nn\n",
        "from tqdm import tqdm #space 바를 실행시켜주는 라이브러리 \n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "class TransformerChat(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # 전체 단어(음절) 개수\n",
        "        self.vocab_size = config[\"vocab_size\"]\n",
        "\n",
        "        # 단어(음절) 벡터 크기\n",
        "        self.embedding_size = config['embedding_size']\n",
        "\n",
        "        # Transformer의 Attention Head 개수\n",
        "        self.num_heads = config['num_heads']\n",
        "\n",
        "        # Transformer Encoder의 Layer 수\n",
        "        self.num_encoder_layers = config['num_encoder_layers']\n",
        "\n",
        "        # Transformer Decoder의 Layer 수\n",
        "        self.num_decoder_layers = config['num_decoder_layers']\n",
        "\n",
        "        # 입력 Sequence의 최대 길이\n",
        "        self.max_length = config['max_length']\n",
        "\n",
        "        # Transformer 내부 FNN 크기\n",
        "        self.hidden_size = config['hidden_size']\n",
        "\n",
        "        # Token Embedding Matrix 선언\n",
        "        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_size)\n",
        "\n",
        "        # Transformer Encoder-Decoder 설계(선언)\n",
        "        self.transformer = nn.Transformer(d_model=self.embedding_size, nhead=self.num_heads, num_encoder_layers=self.num_encoder_layers,\n",
        "                                          num_decoder_layers=self.num_decoder_layers, dim_feedforward=self.hidden_size) #transformer에 parameter를 넣어줌\n",
        "       \n",
        "        # 입력 길이 L에 대한 (L X L) mask 생성: 이전 토큰들의 정보만을 반영하기 위한 mask\n",
        "        #       [[1, -inf, -inf, -inf],\n",
        "        #        [1,    1, -inf, -inf],\n",
        "        #               ......\n",
        "        #        [1,    1,    1,    1]]\n",
        "        # 이곳을 채우세요.\n",
        "        self.mask = self.transformer.generate_square_subsequent_mask(self.max_length).cuda()\n",
        "\n",
        "        # 전체 단어 분포로 변환하기 위한 linear\n",
        "        # 이곳을 채우세요.\n",
        "        self.projection_layer = nn.Linear(self.embedding_size,self.vocab_size) #맨 마지막에 decoding할 때\n",
        "\n",
        "    #모델 설계: forward\n",
        "    def forward(self, enc_inputs, dec_inputs): #encoder, decoder 두개 들어감 - seq2seq\n",
        "\n",
        "        # enc_inputs: [batch, seq_len], dec_inputs: [batch, seq_len]\n",
        "        # enc_input_features: [batch, seq_len, emb_size] -> [seq_len, batch, emb_size]\n",
        "        # 이곳을 채우세요.\n",
        "        enc_input_features = self.embeddings(enc_inputs).transpose(0, 1) #모양을 바꿈 0과 1을 바꿈 \n",
        "\n",
        "        # dec_input_features: [batch, seq_len, emb_size] -> [seq_len, batch, emb_size]\n",
        "        # 이곳을 채우세요.\n",
        "        dec_input_features = self.embeddings(dec_inputs).transpose(0, 1)\n",
        "\n",
        "        # dec_output_features: [seq_len, batch, emb_size]\n",
        "        dec_output_features = self.transformer(src=enc_input_features, tgt=dec_input_features, src_mask = self.mask, tgt_mask = self.mask)\n",
        "\n",
        "        # hypothesis : [seq_len, batch, vocab_size]\n",
        "        hypothesis = self.projection_layer(dec_output_features) #output 나온 것을 projection 시킴\n",
        "\n",
        "        return hypothesis"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmYHzEcU8nRp"
      },
      "source": [
        "#사전 읽기\n",
        "# 어휘사전(vocabulary) 생성 함수\n",
        "def load_vocab(file_dir):\n",
        "\n",
        "    with open(file_dir,'r',encoding='utf8') as vocab_file:\n",
        "        char2idx = {}\n",
        "        idx2char = {}\n",
        "        index = 0\n",
        "        for char in vocab_file:\n",
        "            char = char.strip()\n",
        "            char2idx[char] = index\n",
        "            idx2char[index] = char\n",
        "            index+=1\n",
        "\n",
        "    return char2idx, idx2char\n",
        "\n",
        "# 문자 입력열을 인덱스로 변환하는 함수\n",
        "def convert_data2feature(config, input_sequence, char2idx, decoder_input=False):\n",
        "\n",
        "    # 고정 길이 벡터 생성\n",
        "    input_features = np.zeros(config[\"max_length\"], dtype=np.int) #고정길이 세팅하고 나머지는 zero로 바꿈(pad 입력)\n",
        "\n",
        "    if decoder_input:\n",
        "        # Decoder Input은 Target Sequence에서 Right Shift\n",
        "        # Target Sequence :         [\"안\",\"녕\",\"하\",\"세\",\"요\", \"</S>\" ] #decoder의 output 형태\n",
        "        # Decoder Input Sequence :  [\"<S>\", \"안\",\"녕\",\"하\",\"세\",\"요\"] #decoder의 input 형태(한칸씩 밀려있음)\n",
        "        # 이곳을 채우세요.\n",
        "        input_sequence = \" \".join([\"<S>\"] + input_sequence.split()[:-1])\n",
        "\n",
        "    for idx,token in enumerate(input_sequence.split()):\n",
        "        if token in char2idx.keys():\n",
        "            input_features[idx] = char2idx[token]\n",
        "        else:\n",
        "            input_features[idx] = char2idx['<UNK>']\n",
        "\n",
        "    return input_features\n",
        "\n",
        "#데이터 로드\n",
        "# 데이터 읽기 함수\n",
        "def load_dataset(config):\n",
        "\n",
        "    # 어휘사전 읽어오기\n",
        "    char2idx, idx2char = load_vocab(config['vocab_file'])\n",
        "\n",
        "    file_dir = config['train_file']\n",
        "    data_file = open(file_dir,'r',encoding='utf8').readlines() # Q Vt A\n",
        "\n",
        "    # 데이터를 저장하기 위한 리스트 생성\n",
        "    enc_inputs, dec_inputs, dec_outputs = [], [], []\n",
        "\n",
        "    for line in tqdm(data_file):\n",
        "\n",
        "        line = line.strip().split('\\t') #line을 탭으로 분리\n",
        "\n",
        "        input_sequence = line[0]\n",
        "        output_sequence = line[1]\n",
        "\n",
        "        #모든 data를 읽어와서 붙여놓음, list를 다시 append함\n",
        "        enc_inputs.append(convert_data2feature(config, input_sequence, char2idx))\n",
        "        dec_inputs.append(convert_data2feature(config, output_sequence, char2idx, True)) #한칸씩 옆으로 함\n",
        "        dec_outputs.append(convert_data2feature(config, output_sequence, char2idx))\n",
        "\n",
        "    # 전체 데이터를 저장하고 있는 리스트를 텐서 형태로 변환\n",
        "    enc_inputs = torch.tensor(enc_inputs, dtype=torch.long)\n",
        "    dec_inputs = torch.tensor(dec_inputs, dtype=torch.long)\n",
        "    dec_outputs = torch.tensor(dec_outputs, dtype=torch.long)\n",
        "\n",
        "    return enc_inputs, dec_inputs, dec_outputs, char2idx, idx2char #사전 다시 return"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ-bIKnw-1D8"
      },
      "source": [
        "# 텐서를 리스트로 변환하는 함수\n",
        "def tensor2list(input_tensor):\n",
        "    return input_tensor.cpu().detach().numpy().tolist()\n",
        "\n",
        "#평가\n",
        "def do_test(config, model, word2idx, idx2word, input_sequence=\"오늘 약속있으세요?\"):\n",
        "\n",
        "    # 평가 모드 셋팅\n",
        "    model.eval()\n",
        "\n",
        "    # 입력된 문자열의 음절을 공백 단위 토큰으로 변환. 공백은 <SP>로 변환: \"오늘 약속\" -> \"오 늘 <SP> 약 속\"\n",
        "    input_sequence = \" \".join([e if e != \" \" else \"<SP>\" for e in input_sequence])\n",
        "\n",
        "    # 텐서 변환: [1, seq_len]\n",
        "    enc_inputs = torch.tensor([convert_data2feature(config, input_sequence, word2idx)], dtype=torch.long).cuda()\n",
        "    \n",
        "    # input_ids : [1, seq_len] -> 첫번째 디코더 입력 \"<S>\" 만들기\n",
        "    dec_inputs = torch.tensor([convert_data2feature(config, \"\", word2idx, True)], dtype=torch.long).cuda() #한칸 옆으로 이동\n",
        "    \n",
        "    # 시스템 응답 문자열 초기화\n",
        "    response = ''\n",
        "\n",
        "    # 최대 입력 길이 만큼 Decoding Loop\n",
        "    for decoding_step in range(config['max_length']-1):\n",
        "\n",
        "        # dec_outputs: [vocab_size]\n",
        "        dec_outputs = model(enc_inputs, dec_inputs)[decoding_step, 0, :] #vocabulary 분포도로 나옴\n",
        "        # 가장 큰 출력을 갖는 인덱스 얻어오기\n",
        "        dec_output_idx = np.argmax(tensor2list(dec_outputs)) #list로 바꿈\n",
        "\n",
        "        # 생성된 토큰은 dec_inputs에 추가 (첫번째 차원은 배치)\n",
        "        dec_inputs[0][decoding_step+1] = dec_output_idx\n",
        "\n",
        "        # </S> 심볼 생성 시, Decoding 종료\n",
        "        if idx2word[dec_output_idx] == \"</S>\": #/S가 나오면 끝나는 것 - 이거 나올 때까지 생성함\n",
        "            break\n",
        "\n",
        "        # 생성 토큰 추가\n",
        "        response += idx2word[dec_output_idx]\n",
        "    \n",
        "    # <SP>를 공백으로 변환한 후 응답 문자열 출력\n",
        "    print(response.replace(\"<SP>\", \" \")) #공백은 <SP>로 나옴\n",
        "\n",
        "def test(config):\n",
        "\n",
        "    # 어휘사전 읽어오기\n",
        "    word2idx, idx2word = load_vocab(config['vocab_file'])\n",
        "\n",
        "    # Transformer Seq2Seq 모델 객체 생성\n",
        "    model = TransformerChat(config).cuda()\n",
        "\n",
        "    # 학습한 모델 파일로부터 가중치 불러옴\n",
        "    model.load_state_dict(torch.load(os.path.join(config[\"output_dir\"], config[\"trained_model_name\"])))\n",
        "\n",
        "    while(True):\n",
        "        input_sequence = input(\"문장을 입력하세요. (종료는 exit을 입력하세요.) : \")\n",
        "        if input_sequence == 'exit':\n",
        "            break\n",
        "        do_test(config, model, word2idx, idx2word, input_sequence)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow01KJjz-416"
      },
      "source": [
        "def train(config):\n",
        "\n",
        "    # Transformer Seq2Seq 모델 객체 생성\n",
        "    model = TransformerChat(config).cuda()\n",
        "\n",
        "    # 데이터 읽기\n",
        "    enc_inputs, dec_inputs, dec_outputs, word2idx, idx2word = load_dataset(config)\n",
        "\n",
        "    # TensorDataset/DataLoader를 통해 배치(batch) 단위로 데이터를 나누고 셔플(shuffle)\n",
        "    train_features = TensorDataset(enc_inputs, dec_inputs, dec_outputs)\n",
        "    train_dataloader = DataLoader(train_features, shuffle=True, batch_size=config[\"batch_size\"])\n",
        "\n",
        "    # 크로스엔트로피 손실 함수\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 옵티마이저 함수 지정\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learn_rate\"])\n",
        "\n",
        "    for epoch in range(config[\"epoch\"] + 1):\n",
        "\n",
        "        for (step, batch) in enumerate(train_dataloader):\n",
        "\n",
        "            # 학습 모드 셋팅\n",
        "            model.train()\n",
        "          \n",
        "            # batch = (enc_inputs[step], dec_inputs[step], dec_outputs)*batch_size\n",
        "            # .cuda()를 통해 메모리에 업로드\n",
        "            batch = tuple(t.cuda() for t in batch)\n",
        "\n",
        "            # 역전파 변화도 초기화\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            enc_inputs, dec_inputs, dec_outputs = batch\n",
        "\n",
        "            # hypothesis: [seq_len, batch, vocab_size] -> [seq_len*batch, vocab_size]\n",
        "            # 이곳을 채우세요. logits\n",
        "            hypothesis = model(enc_inputs, dec_inputs).view(-1, config[\"vocab_size\"]) #vocab size만큼 2차원으로 바꾸는 것\n",
        "\n",
        "            # labels: [batch, seq_len] -> [seq_len, batch] -> [seq_len(max_length)*batch]\n",
        "            labels = dec_outputs.transpose(0, 1) #1차원 형태로 늘림\n",
        "            labels = labels.reshape(config[\"max_length\"]*dec_inputs.size(0)) #배치 사이즈 바꿈\n",
        "\n",
        "            # 비용 계산 및 역전파 수행: cross_entopy 내부에서 labels를 원핫벡터로 변환 (골드레이블은 항상 1차원으로 입력)\n",
        "            loss = loss_func(hypothesis, labels) #hypotesis는 2차원 labels는 1차원 \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # 200 배치마다 중간 결과 출력\n",
        "            if (step+1)% 200 == 0:\n",
        "                print(\"Current Step : {0:d} / {1:d}\\tCurrent Loss : {2:f}\".format(step+1, int(len(enc_inputs) / config['batch_size']), loss.item()))\n",
        "                # 생성 문장을 확인하기 위한 함수 호출\n",
        "                # do_test(config, model, word2idx, idx2word)\n",
        "\n",
        "        # 에폭마다 가중치 저장\n",
        "        torch.save(model.state_dict(), os.path.join(config[\"output_dir\"], \"epoch_{0:d}.pt\".format(epoch)))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fk1BLZ-VrHe",
        "outputId": "f023b48d-7f26-4dcf-a269-dce694e5b1c1"
      },
      "source": [
        "if(__name__==\"__main__\"):\n",
        "\n",
        "    root_dir = \"/gdrive/My Drive/colab/transformer/chatbot/\"\n",
        "    output_dir = os.path.join(root_dir, \"output\")\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    config = {\"mode\": \"train\",\n",
        "              \"vocab_file\": os.path.join(root_dir, \"vocab.txt\"),\n",
        "              \"train_file\": os.path.join(root_dir, \"train.txt\"),\n",
        "              \"trained_model_name\":\"epoch_{}.pt\".format(10),\n",
        "              \"output_dir\":output_dir,\n",
        "              \"epoch\": 10,\n",
        "              \"learn_rate\":0.00005,\n",
        "              \"num_encoder_layers\": 6,\n",
        "              \"num_decoder_layers\": 6,\n",
        "              \"num_heads\": 4,\n",
        "              \"max_length\": 20,\n",
        "              \"batch_size\": 128,\n",
        "              \"embedding_size\": 256,\n",
        "              \"hidden_size\": 512,\n",
        "              \"vocab_size\": 4427\n",
        "            }\n",
        "\n",
        "    if(config[\"mode\"] == \"train\"):\n",
        "        train(config)\n",
        "    else:\n",
        "        test(config)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/547958 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "100%|██████████| 547958/547958 [00:13<00:00, 40852.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Step : 200 / 1\tCurrent Loss : 2.686655\n",
            "Current Step : 400 / 1\tCurrent Loss : 2.316056\n",
            "Current Step : 600 / 1\tCurrent Loss : 2.226109\n",
            "Current Step : 800 / 1\tCurrent Loss : 2.184789\n",
            "Current Step : 1000 / 1\tCurrent Loss : 2.099642\n",
            "Current Step : 1200 / 1\tCurrent Loss : 1.858665\n",
            "Current Step : 1400 / 1\tCurrent Loss : 1.902092\n",
            "Current Step : 1600 / 1\tCurrent Loss : 1.888651\n",
            "Current Step : 1800 / 1\tCurrent Loss : 1.776722\n",
            "Current Step : 2000 / 1\tCurrent Loss : 1.922590\n",
            "Current Step : 2200 / 1\tCurrent Loss : 1.659875\n",
            "Current Step : 2400 / 1\tCurrent Loss : 1.819949\n",
            "Current Step : 2600 / 1\tCurrent Loss : 1.657218\n",
            "Current Step : 2800 / 1\tCurrent Loss : 1.843254\n",
            "Current Step : 3000 / 1\tCurrent Loss : 1.755751\n",
            "Current Step : 3200 / 1\tCurrent Loss : 1.930790\n",
            "Current Step : 3400 / 1\tCurrent Loss : 1.732228\n",
            "Current Step : 3600 / 1\tCurrent Loss : 1.791657\n",
            "Current Step : 3800 / 1\tCurrent Loss : 1.844764\n",
            "Current Step : 4000 / 1\tCurrent Loss : 1.686824\n",
            "Current Step : 4200 / 1\tCurrent Loss : 1.866206\n",
            "Current Step : 200 / 1\tCurrent Loss : 1.677886\n",
            "Current Step : 400 / 1\tCurrent Loss : 1.643917\n",
            "Current Step : 600 / 1\tCurrent Loss : 1.899444\n",
            "Current Step : 800 / 1\tCurrent Loss : 1.752033\n",
            "Current Step : 1000 / 1\tCurrent Loss : 1.641136\n",
            "Current Step : 1200 / 1\tCurrent Loss : 1.647060\n",
            "Current Step : 1400 / 1\tCurrent Loss : 1.897849\n",
            "Current Step : 1600 / 1\tCurrent Loss : 1.697893\n",
            "Current Step : 1800 / 1\tCurrent Loss : 1.668893\n",
            "Current Step : 2000 / 1\tCurrent Loss : 1.795531\n",
            "Current Step : 2200 / 1\tCurrent Loss : 1.788157\n",
            "Current Step : 2400 / 1\tCurrent Loss : 1.852192\n",
            "Current Step : 2600 / 1\tCurrent Loss : 1.716952\n",
            "Current Step : 2800 / 1\tCurrent Loss : 1.717251\n",
            "Current Step : 3000 / 1\tCurrent Loss : 1.739007\n",
            "Current Step : 3200 / 1\tCurrent Loss : 1.644566\n",
            "Current Step : 3400 / 1\tCurrent Loss : 1.703506\n",
            "Current Step : 3600 / 1\tCurrent Loss : 1.651673\n",
            "Current Step : 3800 / 1\tCurrent Loss : 1.605142\n",
            "Current Step : 4000 / 1\tCurrent Loss : 1.558203\n",
            "Current Step : 4200 / 1\tCurrent Loss : 1.691539\n",
            "Current Step : 200 / 1\tCurrent Loss : 1.688372\n",
            "Current Step : 400 / 1\tCurrent Loss : 1.626561\n",
            "Current Step : 600 / 1\tCurrent Loss : 1.695391\n",
            "Current Step : 800 / 1\tCurrent Loss : 1.466454\n",
            "Current Step : 1000 / 1\tCurrent Loss : 1.699472\n",
            "Current Step : 1200 / 1\tCurrent Loss : 1.707021\n",
            "Current Step : 1400 / 1\tCurrent Loss : 1.630686\n",
            "Current Step : 1600 / 1\tCurrent Loss : 1.626861\n",
            "Current Step : 1800 / 1\tCurrent Loss : 1.632763\n",
            "Current Step : 2000 / 1\tCurrent Loss : 1.599266\n",
            "Current Step : 2200 / 1\tCurrent Loss : 1.573618\n",
            "Current Step : 2400 / 1\tCurrent Loss : 1.520762\n",
            "Current Step : 2600 / 1\tCurrent Loss : 1.624017\n",
            "Current Step : 2800 / 1\tCurrent Loss : 1.755223\n",
            "Current Step : 3000 / 1\tCurrent Loss : 1.671421\n",
            "Current Step : 3200 / 1\tCurrent Loss : 1.606650\n",
            "Current Step : 3400 / 1\tCurrent Loss : 1.500732\n",
            "Current Step : 3600 / 1\tCurrent Loss : 1.607133\n",
            "Current Step : 3800 / 1\tCurrent Loss : 1.659296\n",
            "Current Step : 4000 / 1\tCurrent Loss : 1.580009\n",
            "Current Step : 4200 / 1\tCurrent Loss : 1.683651\n",
            "Current Step : 200 / 1\tCurrent Loss : 1.532846\n",
            "Current Step : 400 / 1\tCurrent Loss : 1.639268\n",
            "Current Step : 600 / 1\tCurrent Loss : 1.577834\n",
            "Current Step : 800 / 1\tCurrent Loss : 1.590519\n",
            "Current Step : 1000 / 1\tCurrent Loss : 1.625252\n",
            "Current Step : 1200 / 1\tCurrent Loss : 1.501267\n",
            "Current Step : 1400 / 1\tCurrent Loss : 1.655319\n",
            "Current Step : 1600 / 1\tCurrent Loss : 1.679950\n",
            "Current Step : 1800 / 1\tCurrent Loss : 1.552106\n",
            "Current Step : 2000 / 1\tCurrent Loss : 1.530942\n",
            "Current Step : 2200 / 1\tCurrent Loss : 1.668372\n",
            "Current Step : 2400 / 1\tCurrent Loss : 1.708185\n",
            "Current Step : 2600 / 1\tCurrent Loss : 1.723070\n",
            "Current Step : 2800 / 1\tCurrent Loss : 1.639576\n",
            "Current Step : 3000 / 1\tCurrent Loss : 1.536269\n",
            "Current Step : 3200 / 1\tCurrent Loss : 1.532120\n",
            "Current Step : 3400 / 1\tCurrent Loss : 1.504081\n",
            "Current Step : 3600 / 1\tCurrent Loss : 1.550055\n",
            "Current Step : 3800 / 1\tCurrent Loss : 1.617552\n",
            "Current Step : 4000 / 1\tCurrent Loss : 1.603199\n",
            "Current Step : 4200 / 1\tCurrent Loss : 1.607537\n",
            "Current Step : 200 / 1\tCurrent Loss : 1.506031\n",
            "Current Step : 400 / 1\tCurrent Loss : 1.570889\n",
            "Current Step : 600 / 1\tCurrent Loss : 1.601616\n",
            "Current Step : 800 / 1\tCurrent Loss : 1.551631\n",
            "Current Step : 1000 / 1\tCurrent Loss : 1.487518\n",
            "Current Step : 1200 / 1\tCurrent Loss : 1.660932\n",
            "Current Step : 1400 / 1\tCurrent Loss : 1.667249\n",
            "Current Step : 1600 / 1\tCurrent Loss : 1.573719\n",
            "Current Step : 1800 / 1\tCurrent Loss : 1.547347\n",
            "Current Step : 2000 / 1\tCurrent Loss : 1.491540\n",
            "Current Step : 2200 / 1\tCurrent Loss : 1.554010\n",
            "Current Step : 2400 / 1\tCurrent Loss : 1.600117\n",
            "Current Step : 2600 / 1\tCurrent Loss : 1.422270\n",
            "Current Step : 2800 / 1\tCurrent Loss : 1.672652\n",
            "Current Step : 3000 / 1\tCurrent Loss : 1.654708\n",
            "Current Step : 3200 / 1\tCurrent Loss : 1.541724\n",
            "Current Step : 3400 / 1\tCurrent Loss : 1.616792\n",
            "Current Step : 3600 / 1\tCurrent Loss : 1.561975\n",
            "Current Step : 3800 / 1\tCurrent Loss : 1.533051\n",
            "Current Step : 4000 / 1\tCurrent Loss : 1.518624\n",
            "Current Step : 4200 / 1\tCurrent Loss : 1.667087\n",
            "Current Step : 200 / 1\tCurrent Loss : 1.515695\n",
            "Current Step : 400 / 1\tCurrent Loss : 1.570865\n",
            "Current Step : 600 / 1\tCurrent Loss : 1.540211\n",
            "Current Step : 800 / 1\tCurrent Loss : 1.537009\n",
            "Current Step : 1000 / 1\tCurrent Loss : 1.626724\n",
            "Current Step : 1200 / 1\tCurrent Loss : 1.524429\n",
            "Current Step : 1400 / 1\tCurrent Loss : 1.567666\n",
            "Current Step : 1600 / 1\tCurrent Loss : 1.511914\n",
            "Current Step : 1800 / 1\tCurrent Loss : 1.406567\n",
            "Current Step : 2000 / 1\tCurrent Loss : 1.637743\n",
            "Current Step : 2200 / 1\tCurrent Loss : 1.604397\n",
            "Current Step : 2400 / 1\tCurrent Loss : 1.700405\n",
            "Current Step : 2600 / 1\tCurrent Loss : 1.573035\n",
            "Current Step : 2800 / 1\tCurrent Loss : 1.466531\n",
            "Current Step : 3000 / 1\tCurrent Loss : 1.680877\n",
            "Current Step : 3200 / 1\tCurrent Loss : 1.533189\n",
            "Current Step : 3400 / 1\tCurrent Loss : 1.548687\n",
            "Current Step : 3600 / 1\tCurrent Loss : 1.547410\n",
            "Current Step : 3800 / 1\tCurrent Loss : 1.441399\n",
            "Current Step : 4000 / 1\tCurrent Loss : 1.607975\n",
            "Current Step : 4200 / 1\tCurrent Loss : 1.513716\n",
            "Current Step : 200 / 1\tCurrent Loss : 1.610302\n",
            "Current Step : 400 / 1\tCurrent Loss : 1.387230\n",
            "Current Step : 600 / 1\tCurrent Loss : 1.677286\n",
            "Current Step : 800 / 1\tCurrent Loss : 1.478884\n",
            "Current Step : 1000 / 1\tCurrent Loss : 1.452506\n",
            "Current Step : 1200 / 1\tCurrent Loss : 1.631996\n",
            "Current Step : 1400 / 1\tCurrent Loss : 1.560904\n",
            "Current Step : 1600 / 1\tCurrent Loss : 1.631095\n",
            "Current Step : 1800 / 1\tCurrent Loss : 1.519297\n",
            "Current Step : 2000 / 1\tCurrent Loss : 1.527401\n",
            "Current Step : 2200 / 1\tCurrent Loss : 1.656256\n",
            "Current Step : 2400 / 1\tCurrent Loss : 1.667985\n",
            "Current Step : 2600 / 1\tCurrent Loss : 1.528567\n",
            "Current Step : 2800 / 1\tCurrent Loss : 1.581403\n",
            "Current Step : 3000 / 1\tCurrent Loss : 1.446201\n",
            "Current Step : 3200 / 1\tCurrent Loss : 1.482223\n",
            "Current Step : 3400 / 1\tCurrent Loss : 1.555022\n",
            "Current Step : 3600 / 1\tCurrent Loss : 1.613694\n",
            "Current Step : 3800 / 1\tCurrent Loss : 1.578639\n",
            "Current Step : 4000 / 1\tCurrent Loss : 1.471160\n",
            "Current Step : 4200 / 1\tCurrent Loss : 1.580892\n",
            "Current Step : 200 / 1\tCurrent Loss : 1.592941\n",
            "Current Step : 400 / 1\tCurrent Loss : 1.488894\n",
            "Current Step : 600 / 1\tCurrent Loss : 1.609415\n",
            "Current Step : 800 / 1\tCurrent Loss : 1.592303\n",
            "Current Step : 1000 / 1\tCurrent Loss : 1.552701\n",
            "Current Step : 1200 / 1\tCurrent Loss : 1.659340\n",
            "Current Step : 1400 / 1\tCurrent Loss : 1.624968\n",
            "Current Step : 1600 / 1\tCurrent Loss : 1.581250\n",
            "Current Step : 1800 / 1\tCurrent Loss : 1.526391\n",
            "Current Step : 2000 / 1\tCurrent Loss : 1.644478\n",
            "Current Step : 2200 / 1\tCurrent Loss : 1.526414\n",
            "Current Step : 2400 / 1\tCurrent Loss : 1.416598\n",
            "Current Step : 2600 / 1\tCurrent Loss : 1.439679\n",
            "Current Step : 2800 / 1\tCurrent Loss : 1.584052\n",
            "Current Step : 3000 / 1\tCurrent Loss : 1.467152\n",
            "Current Step : 3200 / 1\tCurrent Loss : 1.551692\n",
            "Current Step : 3400 / 1\tCurrent Loss : 1.625899\n",
            "Current Step : 3600 / 1\tCurrent Loss : 1.560980\n",
            "Current Step : 3800 / 1\tCurrent Loss : 1.454690\n",
            "Current Step : 4000 / 1\tCurrent Loss : 1.509112\n",
            "Current Step : 4200 / 1\tCurrent Loss : 1.582281\n",
            "Current Step : 200 / 1\tCurrent Loss : 1.517565\n",
            "Current Step : 400 / 1\tCurrent Loss : 1.397844\n",
            "Current Step : 600 / 1\tCurrent Loss : 1.437717\n",
            "Current Step : 800 / 1\tCurrent Loss : 1.641664\n",
            "Current Step : 1000 / 1\tCurrent Loss : 1.558875\n",
            "Current Step : 1200 / 1\tCurrent Loss : 1.499462\n",
            "Current Step : 1400 / 1\tCurrent Loss : 1.496570\n",
            "Current Step : 1600 / 1\tCurrent Loss : 1.423343\n",
            "Current Step : 1800 / 1\tCurrent Loss : 1.407026\n",
            "Current Step : 2000 / 1\tCurrent Loss : 1.488664\n",
            "Current Step : 2200 / 1\tCurrent Loss : 1.494887\n",
            "Current Step : 2400 / 1\tCurrent Loss : 1.639734\n",
            "Current Step : 2600 / 1\tCurrent Loss : 1.614159\n",
            "Current Step : 2800 / 1\tCurrent Loss : 1.497413\n",
            "Current Step : 3000 / 1\tCurrent Loss : 1.409597\n",
            "Current Step : 3200 / 1\tCurrent Loss : 1.476663\n",
            "Current Step : 3400 / 1\tCurrent Loss : 1.473841\n",
            "Current Step : 3600 / 1\tCurrent Loss : 1.592695\n",
            "Current Step : 3800 / 1\tCurrent Loss : 1.404987\n",
            "Current Step : 4000 / 1\tCurrent Loss : 1.505648\n",
            "Current Step : 4200 / 1\tCurrent Loss : 1.557671\n",
            "Current Step : 200 / 1\tCurrent Loss : 1.449810\n",
            "Current Step : 400 / 1\tCurrent Loss : 1.405033\n",
            "Current Step : 600 / 1\tCurrent Loss : 1.463847\n",
            "Current Step : 800 / 1\tCurrent Loss : 1.619630\n",
            "Current Step : 1000 / 1\tCurrent Loss : 1.416713\n",
            "Current Step : 1200 / 1\tCurrent Loss : 1.549086\n",
            "Current Step : 1400 / 1\tCurrent Loss : 1.516551\n",
            "Current Step : 1600 / 1\tCurrent Loss : 1.499257\n",
            "Current Step : 1800 / 1\tCurrent Loss : 1.438535\n",
            "Current Step : 2000 / 1\tCurrent Loss : 1.411486\n",
            "Current Step : 2200 / 1\tCurrent Loss : 1.480861\n",
            "Current Step : 2400 / 1\tCurrent Loss : 1.486919\n",
            "Current Step : 2600 / 1\tCurrent Loss : 1.503188\n",
            "Current Step : 2800 / 1\tCurrent Loss : 1.615415\n",
            "Current Step : 3000 / 1\tCurrent Loss : 1.521658\n",
            "Current Step : 3200 / 1\tCurrent Loss : 1.500421\n",
            "Current Step : 3400 / 1\tCurrent Loss : 1.555612\n",
            "Current Step : 3600 / 1\tCurrent Loss : 1.473270\n",
            "Current Step : 3800 / 1\tCurrent Loss : 1.557281\n",
            "Current Step : 4000 / 1\tCurrent Loss : 1.570394\n",
            "Current Step : 4200 / 1\tCurrent Loss : 1.583514\n",
            "Current Step : 200 / 1\tCurrent Loss : 1.548348\n",
            "Current Step : 400 / 1\tCurrent Loss : 1.517996\n",
            "Current Step : 600 / 1\tCurrent Loss : 1.355809\n",
            "Current Step : 800 / 1\tCurrent Loss : 1.453174\n",
            "Current Step : 1000 / 1\tCurrent Loss : 1.514782\n",
            "Current Step : 1200 / 1\tCurrent Loss : 1.502144\n",
            "Current Step : 1400 / 1\tCurrent Loss : 1.485777\n",
            "Current Step : 1600 / 1\tCurrent Loss : 1.492556\n",
            "Current Step : 1800 / 1\tCurrent Loss : 1.399733\n",
            "Current Step : 2000 / 1\tCurrent Loss : 1.346430\n",
            "Current Step : 2200 / 1\tCurrent Loss : 1.457934\n",
            "Current Step : 2400 / 1\tCurrent Loss : 1.595207\n",
            "Current Step : 2600 / 1\tCurrent Loss : 1.461482\n",
            "Current Step : 2800 / 1\tCurrent Loss : 1.535727\n",
            "Current Step : 3000 / 1\tCurrent Loss : 1.458873\n",
            "Current Step : 3200 / 1\tCurrent Loss : 1.371878\n",
            "Current Step : 3400 / 1\tCurrent Loss : 1.584874\n",
            "Current Step : 3600 / 1\tCurrent Loss : 1.438158\n",
            "Current Step : 3800 / 1\tCurrent Loss : 1.428976\n",
            "Current Step : 4000 / 1\tCurrent Loss : 1.402767\n",
            "Current Step : 4200 / 1\tCurrent Loss : 1.470916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIhLXar0Ta-Y"
      },
      "source": [],
      "execution_count": 26,
      "outputs": []
    }
  ]
}