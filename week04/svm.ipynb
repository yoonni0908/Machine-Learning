{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbltlYE_x4ZX"
      },
      "source": [
        "<h2>개인 구글 드라이브와 colab 연동</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6q5Z9pNGA0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b8adede-8df0-4775-de86-d1a92573217f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\", force_remount=True)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlXCd1UpzDLL"
      },
      "source": [
        "<h2>\"SMSSpamCollection\" 데이터를 읽고 문장과 정답을 분리하여 각 리스트에 저장</h2>\n",
        "\n",
        "<pre>\n",
        "<b>1. 데이터의 형태(SMSSpamCollection)</b>\n",
        "  라벨(스팸 또는 햄) \\t(tab) 문장 \n",
        "  \n",
        "  위와 같은 형태로 저장되어 있음\n",
        "  \n",
        "  예시)\n",
        "    ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
        "    spam\\tCustomer service annoncement. You have a New Years delivery waiting for you. Please call 07046744435 now to arrange delivery\n",
        "    ...\n",
        "  \n",
        "  따라서 입력 데이터를 읽고 \\t을 기준으로 입력 문장을 분리한 후에 문장과 라벨을 각각 x_data, y_data 리스트에 저장\n",
        "  \n",
        "<b>2. 입력 데이터 전체를 사용하지 않고 100개만 추출해서 사용</b>\n",
        "\n",
        "<b>3. x_data, y_data 형태</b>\n",
        "  x_data = [ 문장1, 문장2, 문장3, ... 문장100]\n",
        "  y_data = [ 문장1의 라벨, 문장2의 라벨, 문장3의 라벨, ... 문장100의 라벨]\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUuFzwfHGFrq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cdb636f-1a1f-4f24-fe20-250172b227d2"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "file_path = \"/gdrive/My Drive/colab/svm/SMSSpamCollection.dat\"\n",
        "\n",
        "# 파일 읽기\n",
        "x_data, y_data = [], []\n",
        "with open(file_path,'r',encoding='utf8') as inFile:\n",
        "  lines = inFile.readlines()\n",
        "  \n",
        "lines = lines[:100]  \n",
        "  \n",
        "  #senetence 와 label을 tab으로 구분 \n",
        "  #앞쪽에는 정답레이블, 뒤쪽에는 문장\n",
        "\n",
        "for line in lines:\n",
        "  line = line.strip().split('\\t')\n",
        "  sentence, label = line[1], line[0]\n",
        "  x_data.append(sentence)\n",
        "  y_data.append(label)\n",
        "  \n",
        "print(\"x_data의 개수 : \" + str(len(x_data)))\n",
        "print(\"y_data의 개수 : \" + str(len(y_data)))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_data의 개수 : 100\n",
            "y_data의 개수 : 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R9OE2Cp1jRX"
      },
      "source": [
        "<h2>Tokenizer 라이브러리를 사용하여 입력 문장을 index로 치환</h2>\n",
        "\n",
        "<pre>\n",
        "<b>1. tokenizer.fit_on_texts(data) 함수를 이용하여 각 단어를 index로 치환하기 위한 딕셔너리 생성</b>\n",
        "   생성된 딕셔너리는 tokenizer 객체 안에 저장됨\n",
        "  \n",
        "  tokenizer.fit_on_texts(data)\n",
        "  args\n",
        "    data : 문자열 element를 가지고 있는 리스트\n",
        "  return\n",
        "    X\n",
        "    \n",
        "  딕셔너리 예시)\n",
        "    {'to': 1, 'i': 2, 'you': 3, 'a': 4, 'the': 5, 'and': 6, 'for': 7 ... }\n",
        "    \n",
        "<b>2. tokenizer.texts_to_sequences(data) 함수를 이용하여 문장 안에 있는 단어들을 index로 치환</b>\n",
        "\n",
        "  tokenizer.texts_to_sequence(data)\n",
        "  args\n",
        "    data : 문자열 element를 가지고 있는 리스트\n",
        "  return : \n",
        "    indexing 된 리스트\n",
        "    \n",
        "  indexing 예시)\n",
        "    x_data indexing 하기 전 : Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
        "    x_data indexing 하기 후 : [38, 93, 239, 240, 241, 242, 53, 11, 243, 72, 94, 244, 245, 126, 246, 247, 73, 74, 248, 127]\n",
        "    y_data indexing 하기 전 : ham\n",
        "    y_data indexing 하기 후 : 1\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8kEaEkA02Qz"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEagO2Q0GOBM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "812f64d0-cebd-4928-d59c-e24e42548141"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "stoplist = nltk.corpus.stopwords.words(\"english\")\n",
        "\n",
        "print(stoplist)\n",
        "\n",
        "#불용어 제거한 후의 리스트\n",
        "result = []\n",
        "\n",
        "for sen in x_data:\n",
        "  sen = sen.lower() #대문자를 소문자로 변경\n",
        "  senList = sen.split() #단어 분리\n",
        "\n",
        "  for stopword in stoplist:\n",
        "    while stopword in senList:\n",
        "      senList.remove(stopword) #문장에서 불용어 제거 \n",
        "  sen = \" \".join(senList) #리스트 다시 문장으로 합치기\n",
        "  result.append(sen)\n",
        "\n",
        "print(result)\n",
        "x_data = result\n",
        "\n",
        "# spam, ham 라벨을 대응하는 index로 치환하기위한 딕셔너리\n",
        "label2index_dict = {'spam':0, 'ham':1}\n",
        "\n",
        "# indexing 한 데이터를 넣을 리스트 선언\n",
        "indexing_x_data, indexing_y_data = [], [] #초기화\n",
        "\n",
        "for label in y_data: #숫자로 바꾼것들을 딕셔너리에 넣음\n",
        "  indexing_y_data.append(label2index_dict[label]) #ham이 들어가면 0을 리턴\n",
        "\n",
        "# x_data를 사용하여 딕셔너리 생성\n",
        "tokenizer.fit_on_texts(x_data)    #string을 주면 space 단위로 쪼개고 vocabulary 개수에 숫자를 부여 - 사전을 자동으로 만듬               \n",
        "\n",
        "# x_data에 있는 각 문장의 단어들을 대응하는 index로 치환하고 그 결과값을 indexing_x_data에 저장\n",
        "indexing_x_data = tokenizer.texts_to_sequences(x_data)    #딕셔너리를 활용해서 데이터를 만듬\n",
        "\n",
        "print(\"x_data indexing 하기 전 : \" + str(x_data[0]))\n",
        "print(\"x_data indexing 하기 후 : \" + str(indexing_x_data[0]))\n",
        "print(\"y_data indexing 하기 전 : \" + str(y_data[0]))\n",
        "print(\"y_data indexing 하기 후 : \" + str(indexing_y_data[0]))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "['go jurong point, crazy.. available bugis n great world la e buffet... cine got amore wat...', 'ok lar... joking wif u oni...', \"free entry 2 wkly comp win fa cup final tkts 21st may 2005. text fa 87121 receive entry question(std txt rate)t&c's apply 08452810075over18's\", 'u dun say early hor... u c already say...', 'nah think goes usf, lives around though', \"freemsg hey darling 3 week's word back! i'd like fun still? tb ok! xxx std chgs send, ￡1.50 rcv\", 'even brother like speak me. treat like aids patent.', \"per request 'melle melle (oru minnaminunginte nurungu vettam)' set callertune callers. press *9 copy friends callertune\", 'winner!! valued network customer selected receivea ￡900 prize reward! claim call 09061701461. claim code kl341. valid 12 hours only.', 'mobile 11 months more? u r entitled update latest colour mobiles camera free! call mobile update co free 08002986030', \"i'm gonna home soon want talk stuff anymore tonight, k? i've cried enough today.\", 'six chances win cash! 100 20,000 pounds txt> csh11 send 87575. cost 150p/day, 6days, 16+ tsandcs apply reply hl 4 info', 'urgent! 1 week free membership ￡100,000 prize jackpot! txt word: claim no: 81010 t&c www.dbuk.net lccltd pobox 4403ldnw1a7rw18', \"i've searching right words thank breather. promise wont take help granted fulfil promise. wonderful blessing times.\", 'date sunday will!!', 'xxxmobilemovieclub: use credit, click wap link next txt message click here>> http://wap. xxxmobilemovieclub.com?n=qjkgighjjgcbl', \"oh k...i'm watching here:)\", 'eh u remember 2 spell name... yes did. v naughty make v wet.', 'fine that?s way u feel. that?s way gota b', 'england v macedonia - dont miss goals/team news. txt ur national team 87077 eg england 87077 try:wales, scotland 4txt/u1.20 poboxox36504w45wq 16+', 'seriously spell name?', 'i‘m going try 2 months ha ha joking', 'u pay first lar... da stock comin...', 'aft finish lunch go str lor. ard 3 smth lor. u finish ur lunch already?', 'ffffffffff. alright way meet sooner?', \"forced eat slice. i'm really hungry tho. sucks. mark getting worried. knows i'm sick turn pizza. lol\", 'lol always convincing.', \"catch bus ? frying egg ? make tea? eating mom's left dinner ? feel love ?\", \"i'm back &amp; we're packing car now, i'll let know there's room\", 'ahhh. work. vaguely remember that! feel like? lol', \"wait that's still clear, sure sarcastic that's x want live us\", 'yeah got 2 v apologetic. n fallen actin like spoilt child got caught that. till 2! go there! badly cheers. you?', 'k tell anything you.', 'fear fainting housework did? quick cuppa', 'thanks subscription ringtone uk mobile charged ￡5/month please confirm replying yes no. reply charged', 'yup... ok go home look timings msg u again... xuhui going learn 2nd may lesson 8am', \"oops, i'll let know roommate's done\", 'see letter b car', 'anything lor... u decide...', \"hello! how's saturday go? texting see decided anything tomo. i'm trying invite anything!\", 'pls go ahead watts. wanted sure. great weekend. abiola', 'forget tell ? want , need you, crave ... ... love sweet arabian steed ... mmmmmm ... yummy', '07732584351 - rodger burns - msg = tried call reply sms free nokia mobile + free camcorder. please call 08000930705 delivery tomorrow', 'seeing?', 'great! hope like man well endowed. &lt;#&gt; inches...', 'calls..messages..missed calls', 'get hep b immunisation nigeria.', 'fair enough, anything going on?', \"yeah hopefully, tyler can't could maybe ask around bit\", \"u know stubborn am. even want go hospital. kept telling mark i'm weak sucker. hospitals weak suckers.\", 'thinked me. first time saw class.', 'gram usually runs like &lt;#&gt; , half eighth smarter though gets almost whole second gram &lt;#&gt;', \"k fyi x ride early tomorrow morning he's crashing place tonight\", 'wow. never realized embarassed accomodations. thought liked it, since best could always seemed happy \"the cave\". i\\'m sorry give. i\\'m sorry offered. i\\'m sorry room embarassing.', 'sms. ac sptv: new jersey devils detroit red wings play ice hockey. correct incorrect? end? reply end sptv', 'know mallika sherawat yesterday? find @ &lt;url&gt;', 'congrats! 1 year special cinema pass 2 yours. call 09061209465 now! c suprman v, matrix3, starwars3, etc 4 free! bx420-ip4-5we. 150pm. dont miss out!', \"sorry, i'll call later meeting.\", 'tell reached', 'yes..gauti sehwag odi series.', \"gonna pick $1 burger way home. can't even move. pain killing me.\", 'ha ha ha good joke. girls situation seekers.', 'part checking iq', 'sorry roommates took forever, ok come now?', 'ok lar double check wif da hair dresser already said wun cut v short. said cut look nice.', 'valued customer, pleased advise following recent review mob no. awarded ￡1500 bonus prize, call 09066364589', 'today \"song dedicated day..\" song u dedicate me? send ur valuable frnds first rply me...', 'urgent ur awarded complimentary trip eurodisinc trav, aco&entry41 ￡1000. claim txt dis 87121 18+6*￡1.50(morefrmmob. shracomorsglsuplt)10, ls1 3aj', 'hear new \"divorce barbie\"? comes ken\\'s stuff!', 'plane give month end.', 'wah lucky man... save money... hee...', 'finished class you.', 'hi babe im home wanna something? xx', 'k..k:)where you?how performed?', 'u call now...', 'waiting machan. call free.', 'thats cool. gentleman treat dignity respect.', 'like peoples much:) shy pa.', 'operate &lt;#&gt;', \"here. still looking job. much ta's earn there.\", \"sorry, i'll call later\", 'k. call ah?', 'ok way home hi hi', 'place man', 'yup next stop.', 'call later, network. urgnt, sms me.', \"real u getting yo? need 2 tickets one jacket i'm done. already used multis.\", \"yes started send requests make pain came back i'm back bed. double coins factory too. gotta cash nitros.\", \"i'm really still tonight babe\", 'ela kano.,il download, come wen ur free..', 'yeah do! don‘t stand close tho- you‘ll catch something!', 'sorry pain. ok meet another night? spent late afternoon casualty means done stuff42moro includes time sheets that. sorry.', 'smile pleasure smile pain smile trouble pours like rain smile sum1 hurts u smile becoz someone still loves see u smiling!!', 'please call customer service representative 0800 169 6031 10am-9pm guaranteed ￡1000 cash ￡5000 prize!', 'havent planning buy later. check already lido got 530 show e afternoon. u finish work already?', 'free ringtone waiting collected. simply text password \"mix\" 85069 verify. get usher britney. fml, po box 5249, mk17 92h. 450ppw 16', 'watching telugu movie..wat abt u?', 'see. finish loads loans pay', 'hi. wk ok - hols now! yes bit run. forgot hairdressers appointment four need get home n shower beforehand. cause prob u?\"', 'see cup coffee animation']\n",
            "x_data indexing 하기 전 : go jurong point, crazy.. available bugis n great world la e buffet... cine got amore wat...\n",
            "x_data indexing 하기 후 : [8, 177, 178, 179, 180, 181, 29, 44, 182, 183, 71, 184, 185, 30, 186, 72]\n",
            "y_data indexing 하기 전 : ham\n",
            "y_data indexing 하기 후 : 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tuo4EGVfAZ4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN5GGyQI_IpT"
      },
      "source": [
        "<h2>SVM 모델 학습</h2>\n",
        "\n",
        "<pre>\n",
        "<b>1. 데이터의 문장 길이를 고정된 길이로 변환</b>\n",
        "  예제 코드에서는 60으로 맞추어 변환\n",
        "  \n",
        "  문장 길이가 60 초과인 경우 뒷 부분 제거\n",
        "  문장 길이가 60 미만인 경우 나머지 부분을 0으로 채움\n",
        "  \n",
        "  예시)\n",
        "    문장 길이를 5로 맞추고자 할 경우\n",
        "    \n",
        "    문장 길이가 5보다 큰 경우\n",
        "    문장 : 나는 어제 집에서 저녁으로 김치찌개를 먹었다, indexing_문장 : 38, 93, 239, 240, 241, 242\n",
        "    38, 93, 239, 240, 241, 242 -> 38, 93, 239, 240, 241\n",
        "    \n",
        "    문장 길이가 5보다 작은 경우\n",
        "    문장 : 나는 김치찌개를 좋아해, indexing_문장 : 74, 248, 127\n",
        "    74, 248, 127 -> 74, 248, 127, 0, 0\n",
        "    \n",
        "<b>2. 입력 데이터를 9 대 1 비율로 나누어 학습, 평가에 사용</b>\n",
        "  train_x = [ 문장1, 문장2, 문장3, ... 문장90]\n",
        "  train_y = [ 문장1의 라벨, 문장2의 라벨, 문장3의 라벨, ... 문장90의 라벨]\n",
        "  test_x = [ 문장91, 문장92, 문장93, ... 문장100]\n",
        "  test_y = [ 문장91의 라벨, 문장92의 라벨, 문장93의 라벨, ... 문장100의 라벨]\n",
        "  \n",
        "<b>3. svm.fit(x, y) 함수를 사용하여 SVM 모델 학습</b>\n",
        "  svm.fit(x, y)\n",
        "  args\n",
        "    x : indexing 된 문장들이 있는 리스트\n",
        "    y : x의 각 문장에 대응하는 라벨이 있는 리스트\n",
        "  return : \n",
        "    X\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYNBrDnzGO-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "969a0680-c264-48ae-f2df-2e36a7e05e71"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# 문장의 길이를 max_length으로 맞춰 변환\n",
        "max_length = 60\n",
        "for index in range(len(indexing_x_data)):\n",
        "  length = len(indexing_x_data[index])\n",
        "  \n",
        "  #벡터 같은 길이로 맞춤 - 차원을 맞추기 위함\n",
        "  if(length > max_length):\n",
        "    indexing_x_data[index] = indexing_x_data[index][:max_length]\n",
        "  elif(length < max_length):\n",
        "    indexing_x_data[index] = indexing_x_data[index] + [0]*(max_length-length)\n",
        "    \n",
        "    \n",
        "# 전체 데이터를 9:1의 비율로 나누어 학습 및 평가 데이터로 사용\n",
        "number_of_train = int(len(indexing_x_data)*0.9)\n",
        "\n",
        "train_x = indexing_x_data[:number_of_train]\n",
        "train_y = indexing_y_data[:number_of_train]\n",
        "test_x = indexing_x_data[number_of_train:]\n",
        "test_y = indexing_y_data[number_of_train:]\n",
        "\n",
        "print(\"train_x의 개수 : \" + str(len(train_x)))\n",
        "print(\"train_y의 개수 : \" + str(len(train_y)))\n",
        "print(\"test_x의 개수 : \" + str(len(test_x)))\n",
        "print(\"test_y의 개수 : \" + str(len(test_y)))\n",
        "\n",
        "svm = SVC(kernel='linear', C=1e10) #C가 오버피팅을 막아주기 위한 숫자임\n",
        "svm.fit(train_x, train_y)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_x의 개수 : 90\n",
            "train_y의 개수 : 90\n",
            "test_x의 개수 : 10\n",
            "test_y의 개수 : 10\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=10000000000.0, kernel='linear')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8Ez7hULOckE"
      },
      "source": [
        "<h2>SVM 모델을 이용한 평가</h2>\n",
        "\n",
        "<pre>\n",
        "<b>1. svm.predict(data) 함수를 사용하여 SVM 모델을 이용하여 평가</b>\n",
        "  \n",
        "  svm.predict(data)\n",
        "  args\n",
        "    data : indexing 된 문장들이 있는 리스트\n",
        "  return : \n",
        "    입력 문장들에 대한 모델의 출력 라벨 리스트\n",
        "    \n",
        "<b>2. 성능 측정</b>\n",
        "  정답 라벨과 모델의 출력 라벨을 비교하여 성능 측정\n",
        "  \n",
        "<b>3. tokenizer.sequences_to_texts(data) 함수를 이용하여 indexing 된 데이터를 단어로 치환</b>\n",
        "\n",
        "  tokenizer.sequences_to_texts(data)\n",
        "  args\n",
        "    data : indexing 된 리스트\n",
        "  return : \n",
        "    단어로 치환된 리스트\n",
        "    \n",
        "  예시)\n",
        "    [38, 93, 239, 240, 241, 242, 53, 11, 243, 72, 94, 244, 245, 126, 246, 247, 73, 74, 248, 127] -> Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
        "    \n",
        "<b>4. 입력 문장에 대한 모델의 출력과 정답 출력</b>\n",
        "\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gONe3GnfGQcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e76b6c02-3c82-4e74-e01c-9615cc967807"
      },
      "source": [
        "predict = svm.predict(test_x)\n",
        "\n",
        "correct_count = 0\n",
        "for index in range(len(predict)):\n",
        "  if(test_y[index] == predict[index]):\n",
        "    correct_count += 1\n",
        "    \n",
        "accuracy = 100.0*correct_count/len(test_y) #accuracy 측정 - predict에서 가져옴\n",
        "\n",
        "\n",
        "print(\"Accuracy: \" + str(accuracy))\n",
        "\n",
        "index2label = {0:\"spam\", 1:\"ham\"} #숫자를 다시 문자로 바꿈\n",
        "\n",
        "test_x_word = tokenizer.sequences_to_texts(test_x)\n",
        "\n",
        "for index in range(len(test_x_word)):\n",
        "  print()\n",
        "  print(\"문장 : \", test_x_word[index])\n",
        "  print(\"정답 : \", index2label[test_y[index]])\n",
        "  print(\"모델 출력 : \", index2label[predict[index]])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 60.0\n",
            "\n",
            "문장 :  yeah do don‘t stand close tho you‘ll catch something\n",
            "정답 :  ham\n",
            "모델 출력 :  ham\n",
            "\n",
            "문장 :  sorry pain ok meet another night spent late afternoon casualty means done stuff42moro includes time sheets that sorry\n",
            "정답 :  ham\n",
            "모델 출력 :  ham\n",
            "\n",
            "문장 :  smile pleasure smile pain smile trouble pours like rain smile sum1 hurts u smile becoz someone still loves see u smiling\n",
            "정답 :  ham\n",
            "모델 출력 :  spam\n",
            "\n",
            "문장 :  please call customer service representative 0800 169 6031 10am 9pm guaranteed ￡1000 cash ￡5000 prize\n",
            "정답 :  spam\n",
            "모델 출력 :  ham\n",
            "\n",
            "문장 :  havent planning buy later check already lido got 530 show e afternoon u finish work already\n",
            "정답 :  ham\n",
            "모델 출력 :  spam\n",
            "\n",
            "문장 :  free ringtone waiting collected simply text password mix 85069 verify get usher britney fml po box 5249 mk17 92h 450ppw 16\n",
            "정답 :  spam\n",
            "모델 출력 :  spam\n",
            "\n",
            "문장 :  watching telugu movie wat abt u\n",
            "정답 :  ham\n",
            "모델 출력 :  ham\n",
            "\n",
            "문장 :  see finish loads loans pay\n",
            "정답 :  ham\n",
            "모델 출력 :  ham\n",
            "\n",
            "문장 :  hi wk ok hols now yes bit run forgot hairdressers appointment four need get home n shower beforehand cause prob u\n",
            "정답 :  ham\n",
            "모델 출력 :  spam\n",
            "\n",
            "문장 :  see cup coffee animation\n",
            "정답 :  ham\n",
            "모델 출력 :  ham\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGk8ia1cSc66"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}